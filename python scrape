import requests
from bs4 import BeautifulSoup
import csv
import time

# Part 1: Scraping product listings

base_url = "https://www.amazon.in/s"
query_params = {
    "k": "bags",
    "crid": "2M096C61O4MLT",
    "qid": "1653308124",
    "sprefix": "ba,aps,283",
    "ref": "sr_pg_1",
}

total_pages_to_scrape = 20

product_data = []

for page in range(1, total_pages_to_scrape + 1):
    query_params["page"] = page
    response = requests.get(base_url, params=query_params)
    soup = BeautifulSoup(response.content, "html.parser")

    products = soup.find_all("div", class_="sg-col-inner")

    for product in products:
        product_url = product.find("a", class_="a-link-normal")["href"]
        product_name = product.find("span", class_="a-text-normal").text
        product_price = product.find("span", class_="a-price-symbol").text + product.find("span", class_="a-price-whole").text
        rating = product.find("span", class_="a-icon-alt").text.split()[0]
        num_reviews = product.find("span", {"aria-label": "ratings"}).find("span", class_="a-size-base").text

        product_data.append({
            "Product URL": product_url,
            "Product Name": product_name,
            "Product Price": product_price,
            "Rating": rating,
            "Number of Reviews": num_reviews
        })

    time.sleep(2)  # Be respectful with your scraping, add a delay between requests

# Part 2: Scraping individual product pages

additional_product_data = []

for product_info in product_data:
    product_url = product_info["Product URL"]
    response = requests.get(product_url)
    soup = BeautifulSoup(response.content, "html.parser")

    # Extract the additional information you need from the product page
    description = ...  # Use BeautifulSoup to find the description
    asin = ...  # Use BeautifulSoup to find the ASIN
    product_description = ...  # Use BeautifulSoup to find the product description
    manufacturer = ...  # Use BeautifulSoup to find the manufacturer

    additional_product_data.append({
        "Product URL": product_url,
        "Description": description,
        "ASIN": asin,
        "Product Description": product_description,
        "Manufacturer": manufacturer
    })

    time.sleep(2)  # Be respectful with your scraping, add a delay between requests

# Combine the two sets of data
for i in range(len(product_data)):
    product_data[i].update(additional_product_data[i])

# Export to CSV
csv_filename = "amazon_product_data.csv"
csv_fields = ["Product URL", "Product Name", "Product Price", "Rating", "Number of Reviews", "Description", "ASIN", "Product Description", "Manufacturer"]

with open(csv_filename, mode="w", newline="", encoding="utf-8") as csv_file:
    writer = csv.DictWriter(csv_file, fieldnames=csv_fields)
    writer.writeheader()
    writer.writerows(product_data)
